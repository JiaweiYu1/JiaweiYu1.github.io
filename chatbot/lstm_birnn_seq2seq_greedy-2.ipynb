{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-birnn-seq2seq-greedy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9EH8Vlmh3Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import time\n",
        "import collections\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxHC0CUsXaI8",
        "colab_type": "code",
        "outputId": "8510206d-35aa-41fd-c60c-cfe846429f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/robot/\")\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Colab_Notebooks/robot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzPW2eyOUyF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(words, n_words, atleast=1):\n",
        "  count =[['PAD', 0],['GO',1], ['EOS',2],['UNK',3]]\n",
        "  \n",
        "  counter = collections.Counter(words).most_common(n_words)\n",
        "  counter = [i for i in counter if i[1] >=atleast]\n",
        "  count.extend(counter)\n",
        "  dictionary = dict()\n",
        "  for word, _ in count:\n",
        "    dictionary[word]=len(dictionary)\n",
        "  data = list()\n",
        "  unk_count=0\n",
        "  for word in words:\n",
        "    index = dictionary.get(word, 0)\n",
        "    if index==0:\n",
        "      unk_count+=1\n",
        "    data.append(index)\n",
        "  count[0][1]=unk_count\n",
        "  \n",
        "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "  \n",
        "  return data, count, dictionary, reversed_dictionary\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsZZ9y84UyBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = open('chat_box_dataset/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open('chat_box_dataset/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        "        \n",
        "convs = [ ]\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))\n",
        "    \n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        questions.append(id2line[conv[i]])\n",
        "        answers.append(id2line[conv[i+1]])\n",
        "        \n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return ' '.join([i.strip() for i in filter(None, text.split())])\n",
        "\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "    \n",
        "clean_answers = []    \n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))\n",
        "    \n",
        "min_line_length = 2\n",
        "max_line_length = 5\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "    i += 1\n",
        "\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "i = 0\n",
        "for answer in short_answers_temp:\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "    i += 1\n",
        "\n",
        "question_test = short_questions[5000:5500]\n",
        "answer_test = short_answers[5000:5500]\n",
        "short_questions = short_questions[:5000]\n",
        "short_answers = short_answers[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uZS2Ya3cEs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#short_questions_1 = short_questions\n",
        "#short_answers_1 = short_answers\n",
        "\n",
        "#short_questions, question_test, short_answers, answer_test = train_test_split(short_questions_1, short_answers_1, test_size=0.3, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzU8k8Z0ZQtV",
        "colab_type": "code",
        "outputId": "c3ab7d35-4fd4-4d5e-b978-862f6d0daa3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(short_questions)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZoeTatVZvOw",
        "colab_type": "code",
        "outputId": "2eca009b-bcba-4cb6-b9b4-33853cf252c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "concat_from = ' '.join(short_questions+question_test).split()\n",
        "vocabulary_size_from = len(list(set(concat_from)))\n",
        "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
        "print('vocab from size: %d'%(vocabulary_size_from))\n",
        "print('Most common words', count_from[4:10])\n",
        "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])\n",
        "print('filtered vocab size:',len(dictionary_from))\n",
        "print(\"% of vocab used: {}%\".format(round(len(dictionary_from)/vocabulary_size_from,4)*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab from size: 3155\n",
            "Most common words [('you', 1202), ('i', 803), ('is', 742), ('what', 521), ('it', 447), ('that', 445)]\n",
            "Sample data [7, 37, 289, 41, 56, 46, 13, 20, 79, 376] ['what', 'good', 'stuff', 'she', 'okay', 'they', 'do', 'to', 'hey', 'sweet']\n",
            "filtered vocab size: 3159\n",
            "% of vocab used: 100.13000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inZCd7JkUx-h",
        "colab_type": "code",
        "outputId": "0d818f38-b07e-4b74-c817-690f1036b997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "concat_to = ' '.join(short_answers+answer_test).split()\n",
        "vocabulary_size_to = len(list(set(concat_to)))\n",
        "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
        "print('vocab from size: %d'%(vocabulary_size_to))\n",
        "print('Most common words', count_to[4:10])\n",
        "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])\n",
        "print('filtered vocab size:',len(dictionary_to))\n",
        "print(\"% of vocab used: {}%\".format(round(len(dictionary_to)/vocabulary_size_to,4)*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab from size: 3107\n",
            "Most common words [('you', 1024), ('i', 898), ('is', 626), ('it', 482), ('not', 472), ('what', 380)]\n",
            "Sample data [10, 226, 4, 5, 296, 40, 56, 11, 8, 120] ['the', 'real', 'you', 'i', 'hope', 'so', 'they', 'do', 'not', 'hi']\n",
            "filtered vocab size: 3111\n",
            "% of vocab used: 100.13000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b-s7yOFUx76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GO = dictionary_from['GO']\n",
        "PAD = dictionary_from['PAD']\n",
        "EOS = dictionary_from['EOS']\n",
        "UNK = dictionary_from['UNK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BQ5Ikq4Ux5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(short_answers)):\n",
        "    short_answers[i] += ' EOS'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epDHK2B8adAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Chatbot:\n",
        "    def __init__(self, size_layer, num_layers, embedded_size, \n",
        "                 from_dict_size, to_dict_size, learning_rate, \n",
        "                 batch_size, dropout = 0.5, beam_width = 15):\n",
        "        \n",
        "        def lstm_cell(size, reuse=False):\n",
        "            return tf.nn.rnn_cell.LSTMCell(size, initializer=tf.orthogonal_initializer(),\n",
        "                                           reuse=reuse)\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, None])\n",
        "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
        "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
        "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
        "        batch_size = tf.shape(self.X)[0]\n",
        "        # encoder\n",
        "        encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
        "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
        "        for n in range(num_layers):\n",
        "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = lstm_cell(size_layer // 2),\n",
        "                cell_bw = lstm_cell(size_layer // 2),\n",
        "                inputs = encoder_embedded,\n",
        "                sequence_length = self.X_seq_len,\n",
        "                dtype = tf.float32,\n",
        "                scope = 'bidirectional_rnn_%d'%(n))\n",
        "            encoder_embedded = tf.concat((out_fw, out_bw), 2)\n",
        "        \n",
        "        bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
        "        bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
        "        bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
        "        self.encoder_state = tuple([bi_lstm_state] * num_layers)\n",
        "        \n",
        "        self.encoder_state = tuple(self.encoder_state[-1] for _ in range(num_layers))\n",
        "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
        "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
        "        # decoder\n",
        "        decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
        "        decoder_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(size_layer) for _ in range(num_layers)])\n",
        "        dense_layer = tf.layers.Dense(to_dict_size)\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
        "                inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input),\n",
        "                sequence_length = self.Y_seq_len,\n",
        "                time_major = False)\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                cell = decoder_cells,\n",
        "                helper = training_helper,\n",
        "                initial_state = self.encoder_state,\n",
        "                output_layer = dense_layer)\n",
        "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = training_decoder,\n",
        "                impute_finished = True,\n",
        "                maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
        "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "                embedding = decoder_embeddings,\n",
        "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
        "                end_token = EOS)\n",
        "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                cell = decoder_cells,\n",
        "                helper = predicting_helper,\n",
        "                initial_state = self.encoder_state,\n",
        "                output_layer = dense_layer)\n",
        "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = predicting_decoder,\n",
        "                impute_finished = True,\n",
        "                maximum_iterations = 2 * tf.reduce_max(self.X_seq_len))\n",
        "        self.training_logits = training_decoder_output.rnn_output\n",
        "        self.predicting_ids = predicting_decoder_output.sample_id\n",
        "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
        "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
        "                                                     targets = self.Y,\n",
        "                                                     weights = masks)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "        y_t = tf.argmax(self.training_logits,axis=2)\n",
        "        y_t = tf.cast(y_t, tf.int32)\n",
        "        self.prediction = tf.boolean_mask(y_t, masks)\n",
        "        mask_label = tf.boolean_mask(self.Y, masks)\n",
        "        correct_pred = tf.equal(self.prediction, mask_label)\n",
        "        correct_index = tf.cast(correct_pred, tf.float32)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn5oRxbhac9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_layer = 256\n",
        "num_layers = 2\n",
        "embedded_size = 128\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "epoch = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQHVqVgac6N",
        "colab_type": "code",
        "outputId": "d4792660-3848-4e34-cbca-d01017cf753f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "model = Chatbot(size_layer, num_layers, embedded_size, len(dictionary_from), \n",
        "                len(dictionary_to), learning_rate,batch_size)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 10:02:54.886035 140572386813824 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "reduction_indices is deprecated, use axis instead\n",
            "W0629 10:02:54.913636 140572386813824 deprecation.py:323] From <ipython-input-10-8309c8bf5b72>:8: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0629 10:02:54.915598 140572386813824 deprecation.py:323] From <ipython-input-10-8309c8bf5b72>:25: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "W0629 10:02:54.918969 140572386813824 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0629 10:02:55.010646 140572386813824 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0629 10:02:55.607496 140572386813824 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0629 10:02:56.223994 140572386813824 deprecation.py:323] From <ipython-input-10-8309c8bf5b72>:38: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0629 10:02:56.654017 140572386813824 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0629 10:02:57.058193 140572386813824 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwxSmWZWac3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_idx(corpus, dic):\n",
        "    X = []\n",
        "    for i in corpus:\n",
        "        ints = []\n",
        "        for k in i.split():\n",
        "            ints.append(dic.get(k,UNK))\n",
        "        X.append(ints)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIxa_E2Mac03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = str_idx(short_questions, dictionary_from)\n",
        "Y = str_idx(short_answers, dictionary_to)\n",
        "X_test = str_idx(question_test, dictionary_from)\n",
        "Y_test = str_idx(answer_test, dictionary_from)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5JqmgyVmzap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    padded_seqs = []\n",
        "    seq_lens = []\n",
        "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
        "    for sentence in sentence_batch:\n",
        "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
        "        seq_lens.append(len(sentence))\n",
        "    return padded_seqs, seq_lens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7vxmbRvmzX4",
        "colab_type": "code",
        "outputId": "86942eb0-5244-455f-d541-5988094a69da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "for i in range(epoch):\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    score=0\n",
        "    for k in range(0, len(short_questions), batch_size):\n",
        "        index = min(k+batch_size, len(short_questions))\n",
        "        batch_x, seq_x = pad_sentence_batch(X[k: index], PAD)\n",
        "        batch_y, seq_y = pad_sentence_batch(Y[k: index], PAD)\n",
        "        predicted, accuracy,loss, _ = sess.run([model.predicting_ids, \n",
        "                                                model.accuracy, model.cost, model.optimizer], \n",
        "                                      feed_dict={model.X:batch_x,\n",
        "                                                model.Y:batch_y})\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "        \n",
        "        \n",
        "        for j in range(len(batch_x)):          \n",
        "          predicted_sent = [rev_dictionary_to[n] for n in predicted[j] if n not in[0,1,2,3]]\n",
        "          real_sent = [[rev_dictionary_to[n] for n in batch_y[j] if n not in[0,1,2,3]]]\n",
        "          score += sentence_bleu(real_sent, predicted_sent)\n",
        "          \n",
        "    print(score)\n",
        "    total_loss /= (len(short_questions) / batch_size)\n",
        "    total_accuracy /= (len(short_questions) / batch_size)\n",
        "    score /= len(short_questions)\n",
        "    print('epoch: %d, avg loss: %f, avg accuracy: %f, avg acore: %f'%(i+1, total_loss, total_accuracy, score))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3732.589538493507\n",
            "epoch: 1, avg loss: 0.646088, avg accuracy: 0.880995, avg acore: 0.746518\n",
            "4103.9073114197545\n",
            "epoch: 2, avg loss: 0.437514, avg accuracy: 0.928144, avg acore: 0.820781\n",
            "4289.607706498\n",
            "epoch: 3, avg loss: 0.333964, avg accuracy: 0.949803, avg acore: 0.857922\n",
            "4400.291207414695\n",
            "epoch: 4, avg loss: 0.268856, avg accuracy: 0.961422, avg acore: 0.880058\n",
            "4482.797457672601\n",
            "epoch: 5, avg loss: 0.221317, avg accuracy: 0.969527, avg acore: 0.896559\n",
            "4524.359484561284\n",
            "epoch: 6, avg loss: 0.189963, avg accuracy: 0.973190, avg acore: 0.904872\n",
            "4531.855420199022\n",
            "epoch: 7, avg loss: 0.169017, avg accuracy: 0.975024, avg acore: 0.906371\n",
            "4548.166175983823\n",
            "epoch: 8, avg loss: 0.153797, avg accuracy: 0.976103, avg acore: 0.909633\n",
            "4553.954335467025\n",
            "epoch: 9, avg loss: 0.148050, avg accuracy: 0.975964, avg acore: 0.910791\n",
            "4542.066777180527\n",
            "epoch: 10, avg loss: 0.141518, avg accuracy: 0.975574, avg acore: 0.908413\n",
            "4521.898772481859\n",
            "epoch: 11, avg loss: 0.134517, avg accuracy: 0.974885, avg acore: 0.904380\n",
            "4505.814908077138\n",
            "epoch: 12, avg loss: 0.135333, avg accuracy: 0.973619, avg acore: 0.901163\n",
            "4487.3581904736975\n",
            "epoch: 13, avg loss: 0.132791, avg accuracy: 0.972724, avg acore: 0.897472\n",
            "4462.9565548951605\n",
            "epoch: 14, avg loss: 0.137979, avg accuracy: 0.970593, avg acore: 0.892591\n",
            "4434.469351406378\n",
            "epoch: 15, avg loss: 0.135765, avg accuracy: 0.969803, avg acore: 0.886894\n",
            "4410.6595956557285\n",
            "epoch: 16, avg loss: 0.128862, avg accuracy: 0.969889, avg acore: 0.882132\n",
            "4403.628228896038\n",
            "epoch: 17, avg loss: 0.128822, avg accuracy: 0.968899, avg acore: 0.880726\n",
            "4381.760864842936\n",
            "epoch: 18, avg loss: 0.129997, avg accuracy: 0.968182, avg acore: 0.876352\n",
            "4371.210196529582\n",
            "epoch: 19, avg loss: 0.130375, avg accuracy: 0.967651, avg acore: 0.874242\n",
            "4353.994510420403\n",
            "epoch: 20, avg loss: 0.127410, avg accuracy: 0.967402, avg acore: 0.870799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWZ7FDC8S3L4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AQVCfZBS3Io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b80e3357-8e81-458a-d8d0-3f370489ee1b"
      },
      "source": [
        "total_loss, total_accuracy = 0, 0\n",
        "score=0\n",
        "for k in range(0, len(question_test), batch_size):\n",
        "  index = min(k+batch_size, len(question_test))\n",
        "  batch_x, seq_x = pad_sentence_batch(X_test[:batch_size], PAD)\n",
        "  batch_y, seq_y = pad_sentence_batch(Y_test[:batch_size], PAD)\n",
        "  predicted, accuracy,loss, _ = sess.run([model.predicting_ids, \n",
        "                                                model.accuracy, model.cost, model.optimizer], \n",
        "                                      feed_dict={model.X:batch_x,\n",
        "                                                model.Y:batch_y})\n",
        "  total_loss += loss\n",
        "  total_accuracy += accuracy\n",
        "        \n",
        "        \n",
        "  for j in range(len(batch_x)):          \n",
        "    predicted_sent = [rev_dictionary_to[n] for n in predicted[j] if n not in[0,1,2,3]]\n",
        "    real_sent = [[rev_dictionary_to[n] for n in batch_y[j] if n not in[0,1,2,3]]]\n",
        "    score += sentence_bleu(real_sent, predicted_sent)\n",
        "          \n",
        "print(score)\n",
        "total_loss /= (len(question_test) / batch_size)\n",
        "total_accuracy /= (len(question_test) / batch_size)\n",
        "score /= len(question_test)\n",
        "print('avg loss: %f, avg accuracy: %f, avg acore: %f'%(total_loss, total_accuracy, score))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "340.32046285884144\n",
            "avg loss: 0.313962, avg accuracy: 0.970483, avg acore: 0.680641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqym3_qtS3FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jrKoUsEjsv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAO3TZPRmzVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(batch_x)):\n",
        "    print('row %d'%(i+1))\n",
        "    print('QUESTION:',' '.join([rev_dictionary_from[n] for n in batch_x[i] if n not in [0,1,2,3]]))\n",
        "    print('REAL ANSWER:',' '.join([rev_dictionary_to[n] for n in batch_y[i] if n not in[0,1,2,3]]))\n",
        "    print('PREDICTED ANSWER:',' '.join([rev_dictionary_to[n] for n in predicted[i] if n not in[0,1,2,3]]),'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1YsDtQmzS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_x, seq_x = pad_sentence_batch(X_test[:batch_size], PAD)\n",
        "batch_y, seq_y = pad_sentence_batch(Y_test[:batch_size], PAD)\n",
        "predicted = sess.run(model.predicting_ids, feed_dict={model.X:batch_x,model.X_seq_len:seq_x})\n",
        "\n",
        "for i in range(len(batch_x)):\n",
        "    print('row %d'%(i+1))\n",
        "    print('QUESTION:',' '.join([rev_dictionary_from[n] for n in batch_x[i] if n not in [0,1,2,3]]))\n",
        "    print('REAL ANSWER:',' '.join([rev_dictionary_to[n] for n in batch_y[i] if n not in[0,1,2,3]]))\n",
        "    print('PREDICTED ANSWER:',' '.join([rev_dictionary_to[n] for n in predicted[i] if n not in[0,1,2,3]]),'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}